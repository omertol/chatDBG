{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from langchain_mistralai.embeddings import MistralAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.chains import create_retrieval_chain\n",
    "import pandas as pd\n",
    "import os\n",
    "from langchain.docstore.document import Document\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "import numpy as np\n",
    "import pickle\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore.in_memory import InMemoryDocstore\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "pickle_file = os.path.join('/home/noaoh/chatDBG/RAG', 'first_data_batch.pkl')\n",
    "data = pd.read_pickle(pickle_file)\n",
    "# file_path = \"bg_arc_output.xlsx\"\n",
    "# docs = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataframe rows to LangChain Documents\n",
    "docs = [\n",
    "    Document(page_content=row['combined'], metadata={\"id\": idx})\n",
    "    for idx, row in data.iterrows()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,  # Maximum tokens per chunk\n",
    "    chunk_overlap=50,  # Overlap between chunks to preserve context\n",
    ")\n",
    "\n",
    "# Split documents into chunks\n",
    "chunked_docs = []\n",
    "for doc in docs:\n",
    "    chunks = text_splitter.split_text(doc.page_content)\n",
    "    # Create new Document objects for each chunk and preserve metadata\n",
    "    chunked_docs.extend([Document(page_content=chunk, metadata=doc.metadata) for chunk in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at dicta-il/dictabert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the embeddings model\n",
    "# class HuggingFaceEmbeddings:\n",
    "#     def __init__(self, model_name):\n",
    "#         self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#         self.model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "#     def embed_text(self, text):\n",
    "#         inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "#         with torch.no_grad():\n",
    "#             outputs = self.model(**inputs)\n",
    "#         return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# # Use AlephBERT\n",
    "# hf_model = HuggingFaceEmbeddings(\"onlplab/alephbert-base\")\n",
    "# \"Davlan/xlm-roberta-large-finetuned-hebrew\" -- another option\n",
    "\n",
    "\n",
    "# class DictaBERTEmbeddings:\n",
    "#     def __init__(self, model_name=\"dicta-il/dictabert\"):\n",
    "#         self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "#         self.model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "#     def embed_text(self, text):\n",
    "#         # Add explicit max_length and truncation\n",
    "#         inputs = self.tokenizer(\n",
    "#             text,\n",
    "#             return_tensors=\"pt\",\n",
    "#             padding=True,\n",
    "#             truncation=True,\n",
    "#             max_length=512  # Explicitly set the max token length\n",
    "#         )\n",
    "#         with torch.no_grad():\n",
    "#             outputs = self.model(**inputs)\n",
    "#         # Use the mean pooling of the last hidden state for embeddings\n",
    "#         return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# # Initialize DictaBERT embeddings\n",
    "# dicta_emb_model = DictaBERTEmbeddings(model_name=\"dicta-il/dictabert\")\n",
    "\n",
    "# # Generate embeddings for all documents\n",
    "# def embed_documents_with_dicta(documents, model):\n",
    "#     return [model.embed_text(doc.page_content) for doc in documents]\n",
    "\n",
    "\n",
    "class DictaBERTEmbeddings:\n",
    "    def __init__(self, model_name=\"dicta-il/dictabert\"):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def embed_text(self, text_batch):\n",
    "        inputs = self.tokenizer(\n",
    "            text_batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512\n",
    "        ).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "        return outputs.last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "\n",
    "# Initialize the embedding model\n",
    "dicta_model = DictaBERTEmbeddings()\n",
    "\n",
    "# Generate embeddings in batches\n",
    "def embed_documents_in_batches(documents, model, batch_size=16):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch_docs = documents[i:i + batch_size]\n",
    "        batch_texts = [doc.page_content for doc in batch_docs]\n",
    "        batch_embeddings = model.embed_text(batch_texts)\n",
    "        embeddings.extend(batch_embeddings)\n",
    "    return np.array(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings\n",
    "embeddings = embed_documents_in_batches(chunked_docs, dicta_model, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings saved to embeddings.pkl\n"
     ]
    }
   ],
   "source": [
    "# Save embeddings to a file\n",
    "with open(\"embeddings.pkl\", \"wb\") as f:\n",
    "    pickle.dump(embeddings, f)\n",
    "\n",
    "print(\"Embeddings saved to embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"embeddings.pkl\", \"rb\") as f:\n",
    "    embeddings = pickle.load(f)\n",
    "\n",
    "print(\"Embeddings loaded from embeddings.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Pair texts with their embeddings\n",
    "text_embedding_pairs = [(doc.page_content, embedding) for doc, embedding in zip(chunked_docs, embeddings)]\n",
    "\n",
    "# # Step 2: Create an in-memory document store\n",
    "# docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(chunked_docs)})\n",
    "\n",
    "# # Step 3: Create FAISS index from text and embeddings\n",
    "# faiss_index = FAISS.from_embeddings(\n",
    "#     text_embeddings=text_embedding_pairs,  # Pair texts with their embeddings\n",
    "#     embedding=dicta_model.embed_text,     # Embedding function for queries\n",
    "#     docstore=docstore,                    # Document store\n",
    "#     index_to_docstore_id={i: str(i) for i in range(len(chunked_docs))}  # Mapping\n",
    "# )\n",
    "\n",
    "# # Step 4: Save the FAISS index\n",
    "# faiss_index.save_local(\"faiss_index\")\n",
    "\n",
    "docstore = InMemoryDocstore({str(i): doc for i, doc in enumerate(chunked_docs)})\n",
    "faiss_index = FAISS.from_embeddings(\n",
    "    text_embeddings=[(doc.page_content, embedding) for doc, embedding in zip(chunked_docs, embeddings)],\n",
    "    embedding=dicta_model.embed_text,\n",
    "    docstore=docstore,\n",
    "    index_to_docstore_id={i: str(i) for i in range(len(chunked_docs))}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    }
   ],
   "source": [
    "# Load the FAISS index\n",
    "faiss_index = FAISS.load_local(\n",
    "    folder_path=\"faiss_index\", \n",
    "    embeddings=dicta_model.embed_text,  # Pass the embedding function\n",
    "    allow_dangerous_deserialization=True  # Enable this to load the pickle\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document content: דבר קשה ואין דבר גדול וחיוני מאשר הפרחת המרחבים האלה, וחברי יטבתה יכולים לספר לכם מה שנעשה שם ב-6 השנים האחרונות. אבל יטבתה אינה מדבר טפוסי, זהו אואזיס. שם יש כל הנתונים למשק פורח חלוצי - לא אומר כמו דגניה, אבל לא פחות מאשר בתל-יוסף. אבל ראיתי מה שעשו צעירים אחרים במדבר ממש, שלא היתה בה אף טפת מים, אף סימן של ירק, אף עץ אחד. רק שממה ערה וערומה. ועכשיו יש שם מרעה, יש שם נטיעות, יש שם לולים, ועדר צאן וסוסים - וגם מלאכת בחשבת של שטיחים. כי הדרום והנגב לא יבנו על חקלאות בלבד, אלא רק על משק מעורב, על חקלאות,\n",
      "Document content: נדמה לי שאנחנו עושים הסיסמאות שלנו פלסתר, אנחנו קוראים לצעירים מן העיר שילכו אל הכפר, יש צעירים שנשמעים והולכים, אבל מה היא הדוגמה שאנחנו מראים להם, כשהמרכז החקלאי יושב בתל אביב, מה הוא עושה בתל אביב? יהיה איש אחד בתל אביב - גם תל אביב זה משהו, יש בה \"רבבות אדם ובהמה רבה\" כמו שכתוב ביונה, יש גם משקים סביב תל אביב ונחוץ שגם פה יהיה למי לפנות, נחוץ שגם בירושלים יהיה למי לפנות, אבל מדוע לא יוכלו אנשי הגליל לפנות למוסד בנצרת, ואנשי הדרום לפנות לאיש בבאר שבע?\n",
      "Document content: השטח הכי גדול הפנוי זהו הנגב. לא מספיק רק ללכת לשם, בנגב יש אדמה, אבל אין מים. אתם יודעים שבלי מים אי אפשר לעשות הרבה באדמה, ייתכן שיש מים מתחת לאדמה, אבל לזה צריכים לעשות נסיונות, זה בכלל לא כל כך קל, גם יכולים לא למצוא במקום זה, צריכים לעשות הקדיחה במקום אחר, ואם גם נניח שלא מוצאים גם במקום שני, יש פתרון לשאלה: אין מים בנגב, אבל יש מים במקום אחר, יש מים בצפון, ועל ידי פלמ\"ח אי אפשר להעביר את המים מהצפון לדרום, לשם זה נחוצה מדינה יהודית - להביא מים מהחצבאני, מהחרמון, מהליטאני לקורונופה(?), כל הבחורים לא\n",
      "Document content: גם האוצרות הטבעיים המעטים שיש בארצנו נמצאים רובם בנגב, קודם כל בים המלח והערבות הנגב נמצאים מרבצי הפוספטים. איכלוס הנגב  זו הבעיה הגדולה ביותר שעומד בפנינו ב-20 השנים הקרובות ודבר זה לא יהיה בלי שני דברים – בלי ידע רב ובלי חלוציות יוצרת.\n",
      "\n",
      "לפני 6-7 שנים עלו 30 יהודים מגטו יהודי ממרוקו, באו לשממה שאין בה מים, אף עץ אחד. הושיבו אותם שם, והמקום הזה במשך 6-7 שנים התפתח והפך לעיירה של.... איש, וראש העיר, כשאני נפגש איתו, יש לו רק תביעה אחת: תבנה עוד 200 בתים בשביל אנשים מהצפון הרוצים לבוא אלינו.\n",
      "Document content: שני שלישי הארץ שהגיע תורם עכשיו בדרום, הם שונים בתכלית, הגשמים שם מועטים וכל מה שמדרימים הגשמים מתמעטים והולכים עד שאתה מגיע לשטח שבכלל אין בו מים ובמקרה הטוב ביותר יורדים 110 מ\"מ לשנה, אני מדבר לא רק על הנגב אלא גם על דרום יהודה (עד באר שבע זהו דרום, מדרום לבאר שבע - זהו נגב), שדה בוקר היא בנגב הצפוני ורק יוטבתה היא בנגב הדרומי, אני מדבר לא על הנגב כעת, אלא על הדרום, כי עדיין לא הגיעה השעה להתיישבות המונים בנגב. לפי שעה אנחנו עומדים בהתיישבותנו רק בקצה דרום יהודה הצפוני, כמעט כל השטח המזרחי של הדרום ריק,\n"
     ]
    }
   ],
   "source": [
    "query = \"מה יש בנגב?\"\n",
    "\n",
    "query_embedding = dicta_model.embed_text(query)\n",
    "\n",
    "# בצע חיפוש ישיר עם FAISS\n",
    "distances, indices = faiss_index.index.search(query_embedding, k=5)  # 5 מסמכים רלוונטיים\n",
    "\n",
    "# מיפוי התוצאות למסמכים\n",
    "relevant_docs = [chunked_docs[i] for i in indices[0] if i != -1]  # התעלמות מתוצאות ריקות\n",
    "\n",
    "# הדפסת המסמכים הרלוונטיים\n",
    "for doc in relevant_docs:\n",
    "    print(f\"Document content: {doc.page_content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple prompt for the RAG chain\n",
    "prompt_template = \"\"\"\n",
    "Use the following documents to answer the question.\n",
    "If you don't know the answer, say \"I don't know.\"\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\n",
    "\"\"\"\n",
    "retrieval_prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'page_content'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Split text into chunks \u001b[39;00m\n\u001b[1;32m      2\u001b[0m text_splitter \u001b[38;5;241m=\u001b[39m RecursiveCharacterTextSplitter()\n\u001b[0;32m----> 3\u001b[0m documents \u001b[38;5;241m=\u001b[39m \u001b[43mtext_splitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Define the embedding model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m MistralAIEmbeddings(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistral-embed\u001b[39m\u001b[38;5;124m\"\u001b[39m, mistral_api_key\u001b[38;5;241m=\u001b[39mapi_key)\n",
      "File \u001b[0;32m~/.conda/envs/my_env/lib/python3.10/site-packages/langchain_text_splitters/base.py:94\u001b[0m, in \u001b[0;36mTextSplitter.split_documents\u001b[0;34m(self, documents)\u001b[0m\n\u001b[1;32m     92\u001b[0m texts, metadatas \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents:\n\u001b[0;32m---> 94\u001b[0m     texts\u001b[38;5;241m.\u001b[39mappend(\u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_content\u001b[49m)\n\u001b[1;32m     95\u001b[0m     metadatas\u001b[38;5;241m.\u001b[39mappend(doc\u001b[38;5;241m.\u001b[39mmetadata)\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_documents(texts, metadatas\u001b[38;5;241m=\u001b[39mmetadatas)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'page_content'"
     ]
    }
   ],
   "source": [
    "# Split text into chunks \n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "# Define the embedding model\n",
    "embeddings = MistralAIEmbeddings(model=\"mistral-embed\", mistral_api_key=api_key)\n",
    "# Create the vector store \n",
    "vector = FAISS.from_documents(documents, embeddings)\n",
    "# Define a retriever interface\n",
    "retriever = vector.as_retriever()\n",
    "# Define LLM\n",
    "model = ChatMistralAI(mistral_api_key=api_key)\n",
    "# Define prompt template\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\"\"\")\n",
    "\n",
    "# Create a retrieval chain to answer questions\n",
    "document_chain = create_stuff_documents_chain(model, prompt)\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "response = retrieval_chain.invoke({\"input\": \"What were the two main things the author worked on before college?\"})\n",
    "print(response[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
