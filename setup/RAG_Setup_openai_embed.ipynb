{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c2853b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores.faiss import DistanceStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b75c8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {DEVICE}')\n",
    "\n",
    "DATA_PATH = './data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27a4bdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "pickle_file = os.path.join(DATA_PATH, 'prepd_data.pkl')\n",
    "data = pd.read_pickle(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0b25f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2354\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47e2fcbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>book_id</th>\n",
       "      <th>headline</th>\n",
       "      <th>combined_info</th>\n",
       "      <th>combined_info_pp</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>241112</td>\n",
       "      <td>חזון עתיד מתוקן, חגי ישראל, חזון הנביאים, תנופ...</td>\n",
       "      <td>ד. בן-גוריון    י\"ח אייר תש\"ט 17.5.49\\n\\n\\n\\nא...</td>\n",
       "      <td>ד. בן-גוריון י\"ח אייר תש\"ט 17.5.49 אין עמים רב...</td>\n",
       "      <td>he</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>241170</td>\n",
       "      <td>israel and the middle east, חברות ישראל באו\"ם,...</td>\n",
       "      <td>For Israel and Middle East\\n\\nBy David Ben-Gur...</td>\n",
       "      <td>For Israel and Middle East By David Ben-Gurion...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>241180</td>\n",
       "      <td>חוק שירות הביטחון</td>\n",
       "      <td>ישיבה ע' ו, 5.9.1949    חוק שירות הבטחון תש\"ט-...</td>\n",
       "      <td>ישיבה ע' ו, 5.9.1949 חוק שירות הבטחון תש\"ט- 19...</td>\n",
       "      <td>he</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>241225</td>\n",
       "      <td>how ambassador Morgenthau saved Palestine jewr...</td>\n",
       "      <td>HOW AMBASSADOR MORGENTHAU SAVED PALESTINE JEWR...</td>\n",
       "      <td>HOW AMBASSADOR MORGENTHAU SAVED PALESTINE JEWR...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>241459</td>\n",
       "      <td>[ח\"א?] במטה המורחב, צה\"ל, מערכת סיני, מלחמת סי...</td>\n",
       "      <td>12.12.57 ח\"א במטה המורחב\\n\\nאני מודה, שהדברים ...</td>\n",
       "      <td>12.12.57 ח\"א במטה המורחב אני מודה, שהדברים של ...</td>\n",
       "      <td>he</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   book_id                                           headline  \\\n",
       "0   241112  חזון עתיד מתוקן, חגי ישראל, חזון הנביאים, תנופ...   \n",
       "1   241170  israel and the middle east, חברות ישראל באו\"ם,...   \n",
       "2   241180                                  חוק שירות הביטחון   \n",
       "3   241225  how ambassador Morgenthau saved Palestine jewr...   \n",
       "4   241459  [ח\"א?] במטה המורחב, צה\"ל, מערכת סיני, מלחמת סי...   \n",
       "\n",
       "                                       combined_info  \\\n",
       "0  ד. בן-גוריון    י\"ח אייר תש\"ט 17.5.49\\n\\n\\n\\nא...   \n",
       "1  For Israel and Middle East\\n\\nBy David Ben-Gur...   \n",
       "2  ישיבה ע' ו, 5.9.1949    חוק שירות הבטחון תש\"ט-...   \n",
       "3  HOW AMBASSADOR MORGENTHAU SAVED PALESTINE JEWR...   \n",
       "4  12.12.57 ח\"א במטה המורחב\\n\\nאני מודה, שהדברים ...   \n",
       "\n",
       "                                    combined_info_pp lang  \n",
       "0  ד. בן-גוריון י\"ח אייר תש\"ט 17.5.49 אין עמים רב...   he  \n",
       "1  For Israel and Middle East By David Ben-Gurion...   en  \n",
       "2  ישיבה ע' ו, 5.9.1949 חוק שירות הבטחון תש\"ט- 19...   he  \n",
       "3  HOW AMBASSADOR MORGENTHAU SAVED PALESTINE JEWR...   en  \n",
       "4  12.12.57 ח\"א במטה המורחב אני מודה, שהדברים של ...   he  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "781964a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataframe rows to LangChain Documents\n",
    "docs = [\n",
    "    Document(page_content=row['combined_info_pp'], metadata={\"book_id\": row['book_id'], \"headline\": row[\"headline\"], \"source\": \"DBGH\"})\n",
    "    for idx, row in data.iterrows()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da39d7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_by_words(text, max_words, overlap_words):\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "\n",
    "    chunks = []\n",
    "    start_idx = 0\n",
    "    while start_idx < len(words):\n",
    "        end_idx = min(start_idx + max_words, len(words))\n",
    "        chunk = words[start_idx:end_idx]\n",
    "\n",
    "        # Join the words back into text\n",
    "        chunk_text = ' '.join(chunk)\n",
    "        chunks.append(chunk_text)\n",
    "\n",
    "        # Update the starting position with overlap\n",
    "        start_idx += max_words - overlap_words\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "db70f639",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = []\n",
    "idx = 0\n",
    "\n",
    "for doc in docs:\n",
    "    temp_chunks = split_text_by_words(\n",
    "        doc.page_content, max_words=260, overlap_words=35\n",
    "    )\n",
    "    \n",
    "    for chunk in temp_chunks:\n",
    "        new_metadata = dict(doc.metadata) \n",
    "        new_metadata['idx'] = idx \n",
    "        chunks.append(Document(page_content=chunk, metadata=new_metadata))\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "594e85da",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_path = os.path.join(DATA_PATH, \"chunks.pkl\")\n",
    "with open(new_data_path, \"wb\") as file: \n",
    "    pickle.dump(chunks, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc7f807c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 21529\n"
     ]
    }
   ],
   "source": [
    "# Check the number of chunks created\n",
    "print(f\"Number of chunks: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be683836",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"your-api-key\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1fd128b",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b634315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def faiss_from_documents(documents, embedding_model, batch_size=20, delay_sec=5):\n",
    "    \"\"\"Embeds LangChain Documents safely in batches and builds a FAISS index.\"\"\"\n",
    "\n",
    "    texts, metadatas = [], []\n",
    "    for doc in documents:\n",
    "        texts.append(doc.page_content)\n",
    "        metadatas.append(doc.metadata)\n",
    "\n",
    "    all_embeddings = []\n",
    "    all_texts = []\n",
    "    all_metas = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size)):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        batch_metas = metadatas[i:i+batch_size]\n",
    "        try:\n",
    "            batch_embeddings = embedding_model.embed_documents(batch_texts)\n",
    "            all_embeddings.extend(batch_embeddings)\n",
    "            all_texts.extend(batch_texts)\n",
    "            all_metas.extend(batch_metas)\n",
    "        except Exception as e:\n",
    "            print(f\"Error at batch {i}: {e}\")\n",
    "            print(\"Pausing before retry...\")\n",
    "            time.sleep(delay_sec * 2)\n",
    "            continue\n",
    "        time.sleep(delay_sec)\n",
    "\n",
    "    text_embedding_pairs = list(zip(all_texts, all_embeddings))\n",
    "    \n",
    "    # create FAISS index from precomputed embeddings\n",
    "    return FAISS.from_embeddings(\n",
    "        text_embedding_pairs,  \n",
    "        embedding=embedding_model,\n",
    "        metadatas=all_metas,\n",
    "        distance_strategy=DistanceStrategy.COSINE\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3140a4eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1436 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1436/1436 [2:38:58<00:00,  6.64s/it] \n"
     ]
    }
   ],
   "source": [
    "vectorstore = faiss_from_documents(\n",
    "    documents=chunks,\n",
    "    embedding_model=embedding_model,\n",
    "    batch_size=15,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15b0473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save FAISS index\n",
    "vectorstore.save_local(\"./faiss_index_openai_3textlarge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4f3f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FAISS index\n",
    "vectorstore = FAISS.load_local(\"./faiss_index_openai_3textlarge\", \n",
    "                               embedding_model,\n",
    "                            allow_dangerous_deserialization=True\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e5c2a486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21529\n"
     ]
    }
   ],
   "source": [
    "print(len(vectorstore.docstore._dict))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
