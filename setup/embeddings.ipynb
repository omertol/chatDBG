{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b99c01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dded0138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {DEVICE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06699128",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1091881/2590703118.py:4: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embedding_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n"
     ]
    }
   ],
   "source": [
    "api_key = \"YOUR-API-KEY\"\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c070b816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "DATA_PATH = './data/'\n",
    "\n",
    "pickle_file = os.path.join(DATA_PATH, 'prepd_data.pkl')\n",
    "data = pd.read_pickle(pickle_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2a48d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dataframe rows to LangChain Documents\n",
    "docs = [\n",
    "    Document(page_content=row['combined_info_pp'], metadata={\"book_id\": row['book_id'], \"headline\": row[\"headline\"], \"from_date\": row[\"from_date\"], \"to_date\": row[\"to_date\"]})\n",
    "    for idx, row in data.iterrows()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f12ec15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text_by_words(text, max_words, overlap_words):\n",
    "    # Split the text into words\n",
    "    words = text.split()\n",
    "\n",
    "    chunks = []\n",
    "    start_idx = 0\n",
    "    while start_idx < len(words):\n",
    "        end_idx = min(start_idx + max_words, len(words))\n",
    "        chunk = words[start_idx:end_idx]\n",
    "\n",
    "        # Join the words back into text\n",
    "        chunk_text = ' '.join(chunk)\n",
    "        chunks.append(chunk_text)\n",
    "\n",
    "        # Update the starting position with overlap\n",
    "        start_idx += max_words - overlap_words\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f53bd658",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = []\n",
    "idx = 0\n",
    "\n",
    "for doc in docs:\n",
    "    temp_chunks = split_text_by_words(\n",
    "        doc.page_content, max_words=200, overlap_words=20\n",
    "    )\n",
    "    \n",
    "    for chunk in temp_chunks:\n",
    "        new_metadata = dict(doc.metadata) \n",
    "        new_metadata['idx'] = idx \n",
    "        chunks.append(Document(page_content=chunk, metadata=new_metadata))\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f4aa519",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_path = os.path.join(DATA_PATH, \"chunks.pkl\")\n",
    "with open(new_data_path, \"wb\") as file: \n",
    "    pickle.dump(chunks, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c612ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 26579\n"
     ]
    }
   ],
   "source": [
    "# Check the number of chunks created\n",
    "print(f\"Number of chunks: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddb08fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_save_embeddings(documents, embedding_model, batch_size=50, output_path=\"embeddings.pkl\"):\n",
    "    texts = [doc.page_content for doc in documents]\n",
    "    metas = [doc.metadata for doc in documents]\n",
    "\n",
    "    all_embeddings = []\n",
    "    all_texts = []\n",
    "    all_metas = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating embeddings\"):\n",
    "        batch_texts = texts[i:i+batch_size]\n",
    "        batch_metas = metas[i:i+batch_size]\n",
    "\n",
    "        try:\n",
    "            batch_embeddings = embedding_model.embed_documents(batch_texts)\n",
    "        except Exception as e:\n",
    "            print(f\"Error at batch {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "        all_embeddings.extend(batch_embeddings)\n",
    "        all_texts.extend(batch_texts)\n",
    "        all_metas.extend(batch_metas)\n",
    "\n",
    "    with open(output_path, \"wb\") as f:\n",
    "        pickle.dump({\n",
    "            \"texts\": all_texts,\n",
    "            \"metas\": all_metas,\n",
    "            \"embeddings\": all_embeddings\n",
    "        }, f)\n",
    "\n",
    "    print(f\"Saved {len(all_embeddings)} embeddings to {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
