import os
import torch
import pandas as pd
from langchain.vectorstores import FAISS
from langchain.chat_models import ChatOpenAI
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from DictaBERTEmbeddings import DictaBERTEmbeddings
from query_prep import *

# Constants
DATA_PATH = './data/'
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
api_key = "YOUR-API-KEY"

# Global variables
data = None
retriever = None
answer_prompt = None
answer_llm = None
vectorstore = None
qGeneration_llm = None
qGeneration_prompt = None
verifier_prompt = None
verifier_llm = None
judge_llm = None
judge_prompt = None
translation_prompt = None
translation_llm = None

# Helper functions
def initialize_retriever(index_name, embedding_model):
    vectorstore = FAISS.load_local(
        folder_path=index_name,
        embeddings=embedding_model,
        allow_dangerous_deserialization=True
    )
    embeddings_retriever = vectorstore.as_retriever(search_kwargs={"k": 10})

    bm25_retriever = BM25Retriever.from_documents(data)
    bm25_retriever.k = 10

    hybrid_retriever = EnsembleRetriever(
        retrievers=[embeddings_retriever, bm25_retriever],
        weights=[0.6, 0.4]
    )
    return hybrid_retriever


def initialize_llm():
    answer_llm = ChatOpenAI(
        model="gpt-4.1",
        openai_api_key=api_key,
        top_p = 1,
        temperature = 0.7
        )
    qGeneration_llm = ChatOpenAI(model="gpt-4.1",
        openai_api_key=api_key,
        top_p = 1,
        temperature = 0.5
        )
    verifier_llm = ChatOpenAI(
        model="gpt-4.1",
        openai_api_key=api_key,
        top_p = 1,
        temperature = 0
        )
    judge_llm = ChatOpenAI(
        model="gpt-4.1",
        openai_api_key=api_key,
        top_p = 1,
        temperature = 0
        )
    translation_llm = ChatOpenAI(
        model="ft:gpt-4o-mini-2024-07-18:chatdbg:chatdbg-v4:BZxdjxkl",
        top_p=1.0,
        temperature=0.75,
        openai_api_key=api_key
        )
    answer_prompt = PromptTemplate(
        input_variables=["query", "context"],
        template=(
            # Prompt example (The real prompt is more complex)
            """
            Act as David Ben-Gurion, Israel's first PM. 
            1. Context: {context}
            2. User Question: {query}
            Answer the user's question.
            ...
            """
        )
    )
    qGeneration_prompt = PromptTemplate(
        input_variables=["question"],
        template=
        # Prompt example
        """
        You are a helpful assistant. The user may ask a question in either **Hebrew or English**.

        Your task is to refine and rephrase the question according to the following logic:

        ...
        
        ### Original question:
        {question}

        ### Reformulated version(s):
        """
        )
    verifier_prompt = PromptTemplate(
        input_variables=["query", "context"],
        template=
        # Prompt example
        """
        Assess if the context helps answer the query. Choose one rating:

        Strong support – Directly answers the query.

        Partial support – Offers partial or indirect help.

        No support – Not helpful at all.
        """)
    judge_prompt = PromptTemplate(
        input_variables=["query", "context", "answers"],
        template=
        # Prompt example
        """
        You are an expert assistant. You will be given a user question, optional context, and several answers generated by another language model.

        Question:
        {query}

        Context:
        {context}

        Answers:
        {answers}
        Do NOT explain your choice. Do NOT repeat the answer.
        """
        )
    translation_prompt = PromptTemplate(
    input_variables=["user_language", "input"],
    template=(
    # Prompt example
    """
    אתה מחקה את דוד בן-גוריון. תכתוב כמו בן-גוריון היה מדבר — בעברית גבוהה, רצינית, אבל תישאר בשפה של המשתמש. תכתוב בלשון זכר ובפיסוק תקין. אל תמציא שטויות.
    כתוב בשפה: {user_language}

    המשתמש אמר:
    {input}
    """
    ))
    return answer_prompt, answer_llm, qGeneration_prompt, qGeneration_llm, verifier_prompt, verifier_llm, judge_prompt, judge_llm, translation_prompt, translation_llm

# --- Get context for a given document ID (and neighbors from same source) ---
def get_context_with_neighbors(idx, data):
    current_doc = data[idx]
    match_value = current_doc.metadata.get("source")
    
    combined_chunks = []
    for neighbor_idx in [idx]:  # Can be extended to idx ± 1 etc.
        if 0 <= neighbor_idx < len(data):
            neighbor_doc = data[neighbor_idx]
            if neighbor_doc.metadata.get("source") == match_value:
                combined_chunks.append(neighbor_doc.page_content)

    return "\n".join(combined_chunks)

# --- Generate variants of the query ---
def generate_queries(original_query):
    chain = qGeneration_prompt | qGeneration_llm
    output = chain.invoke({"question": original_query})
    text_output = output.content

    queries = [
        line.strip()
        for line in text_output.strip().split("\n")
        if line.strip() and not line.strip().startswith("###")
    ]
    print(queries)
    return queries

# --- Retrieve relevant context and doc IDs ---
def retrieve_contexts(query):
    context_docs = retriever.invoke(query)
    retrieved_ids = [doc.metadata['idx'] for doc in context_docs]
    context_text = "\n\n".join([
        get_context_with_neighbors(idx, data) for idx in retrieved_ids
    ])
    return context_text, retrieved_ids

# --- Check if the context supports the query ---
def is_supported_by_context(query, context):
    decision = (verifier_prompt | verifier_llm).invoke({
        "query": query,
        "context": context
    }).content.strip()
    return "No support" not in decision

# --- Perform self-consistency to get the most robust answer ---
def get_consistent_answer(query, context_text, n_consistency=5):
    def single_call(_):
        max_retries = 3
        for attempt in range(max_retries):
            try:
                return (answer_prompt | answer_llm).invoke({
                    "query": query,
                    "context": context_text
                }).content.strip()
            except RateLimitError:
                if attempt < max_retries - 1:
                    time.sleep(5)
                else:
                    raise

    with ThreadPoolExecutor() as executor:
        answers = list(executor.map(single_call, range(n_consistency)))

    if len(set(answers)) == 1:
        return answers[0]

    formatted_answers = "\n\n".join([
        f"Answer {i+1}:\n{ans}" for i, ans in enumerate(answers)
    ])

    judge_output = (judge_prompt | judge_llm).invoke({
        "query": query,
        "context": context_text,
        "answers": formatted_answers
    }).content.strip()

    match = re.search(r"Answer\s*(\d+)", judge_output)
    if match:
        chosen_index = int(match.group(1)) - 1
        if 0 <= chosen_index < len(answers):
            return answers[chosen_index]

    return answers[0]

def translate_response(text):
    translation_chain = translation_prompt | translation_llm
    response = translation_chain.invoke({"input": text})
    return response.content.strip()

# --- Main function: pipeline to process query and return final answer ---
def answer_query(user_query, to_print=False):
    # First try original query
    context_text, retrieved_ids = retrieve_contexts(user_query)
    
    if is_supported_by_context(user_query, context_text):
        response = get_consistent_answer(user_query, context_text)
        if to_print:
            print(f"User's Question: {user_query}\nFinal Answer: {response}")
            print("\nContext:\n", context_text)
        return response, (", ".join(str(bid) for bid in retrieved_ids))
    # Otherwise try variants
    queries = generate_queries(user_query)
    for q in queries[1:]:
        context_text, retrieved_ids = retrieve_contexts(q)
        if is_supported_by_context(user_query, context_text):
            response = get_consistent_answer(user_query, context_text)
            if to_print:
                print(f"User's Question: {user_query}\nFinal Answer: {response}")
                print("\nContext:\n", context_text)
            DBG_translation = translate_response(response)
            return response, DBG_translation, (", ".join(str(bid) for bid in retrieved_ids))

    # Fallback: no support found
    response = get_consistent_answer(user_query, "")
    if to_print:
        print(f"User's Question: {user_query}\nFinal Answer: {response}")
        print("Context: No relevant context found.")

    DBG_translation = translate_response(response)
    return response, DBG_translation, []

def load_pkl(path):
    pickle_file = os.path.join(DATA_PATH, path)
    return pd.read_pickle(pickle_file)


def initialize_components():
    global data, retriever, vectorstore, answer_prompt, answer_llm, qGeneration_prompt, qGeneration_llm, verifier_prompt, verifier_llm, judge_prompt, judge_llm, translation_prompt, translation_llm
    data = load_pkl('combined_chunks.pkl')
    embedding_model = OpenAIEmbeddings(model="text-embedding-3-large")
    answer_prompt, answer_llm, qGeneration_prompt, qGeneration_llm, verifier_prompt, verifier_llm, judge_prompt, judge_llm, translation_prompt, translation_llm = initialize_llm()
    retriever = initialize_retriever("faiss_index_openai_3textlarge_copy", embedding_model)
    print("Done initialize")
    
if __name__ == "__main__":
    initialize_components()
